namespace StackOverflowRAG.Core.Interfaces;

/// <summary>
/// Service for LLM integration (streaming responses)
/// </summary>
public interface ILlmService
{
    // TODO: Implement in Story 2.3
    // IAsyncEnumerable<string> StreamAnswerAsync(string question, List<DocumentChunk> context, CancellationToken cancellationToken);
}
